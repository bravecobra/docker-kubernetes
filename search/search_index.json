{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kubernetes intro setup","text":"<p>Warning</p> <p>I wanted to figure out how hard it would be to get a kubernetes cluster up and running om a bare metal machine. This setup is by no means secure and should never be used to a production environment. This is only an experiment!</p>"},{"location":"#content","title":"Content","text":"<ul> <li>Install an ESXi on a baremetal machine</li> <li>Create a kubernetes cluster</li> <li>Connecting to the cluster, using kubectl</li> <li>Add a cluster dashboard</li> <li>Install helm</li> <li>Install rancher</li> <li>Install an application on the cluster.</li> </ul>"},{"location":"dashboard/","title":"Install dashboard","text":""},{"location":"dashboard/#install","title":"Install","text":"<p>From https://github.com/kubernetes/dashboard</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml\n</code></pre> <p>Look into https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md</p> <pre><code>kubectl apply -f ./src/dashboard/service-account.yaml  #create an account\nkubectl create -f ./src/dashboard/dashboard-admin.yaml #give the account cluster-admin access\n</code></pre>"},{"location":"dashboard/#using-the-dashboard","title":"Using the dashboard","text":"<p>Run the following to get the token with <code>cluster-admin</code> access</p> <pre><code>kubectl -n kubernetes-dashboard get secret\n</code></pre> <p>Look for <code>admin-user-token-xxxxx</code> and get the token with</p> <pre><code>kubectl -n kubernetes-dashboard describe secret admin-user-token-xxxxx\n</code></pre> <p>or</p> <pre><code>kubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')\n</code></pre> <p>Now proxy the dashboard</p> <pre><code>kubectl proxy\n</code></pre> <p>and use the token return from the previous command to login at dashboard.</p>"},{"location":"esxi/","title":"Kubernetes on ESXi","text":""},{"location":"esxi/#install-esxi","title":"Install ESXi","text":"<p>I installed ESXi on an empty bare metal machine (without an OS) by running the ESXi installer from a boot USB disk.</p> <p>To create that USB boot disk, I used Rufus, adding the ESXi 6.7.0 install ISO.</p> <p></p> <p>I booted the empty machine with the USB disk and followed the installers instructions.</p> <p>Once I had the ESXi up and running,</p> <ul> <li>I added the (free) license key</li> <li>I set the IP address of the ESXi to be fixed. </li> <li>Setup the DNS  </li> </ul>"},{"location":"helm/","title":"Helm","text":""},{"location":"helm/#install-helm-on-the-cluster","title":"Install helm on the cluster","text":"<pre><code>kubectl create -f ./src/helm/rbac-config.yaml\nhelm init --service-account tiller --history-max 200\n</code></pre> <p>To test the correct installation of tiller, verify that the pods has been deployed</p> <pre><code>kubectl get pods --namespace kube-system | grep tiller\n</code></pre> <p>Current version of helm is incompatible with kubernetes 1.16. Some api versions got deprecated. To work around that, update those</p> <pre><code>helm init --service-account tiller --override spec.selector.matchLabels.'name'='tiller',spec.selector.matchLabels.'app'='helm' --output yaml &gt; ./src/helm/tiller.yaml\nsed 's@apiVersion: extensions/v1beta1@apiVersion: apps/v1@'\nkubectl apply -f ./src/helm/tiller.yaml\n</code></pre>"},{"location":"kubectl/","title":"kubectl","text":""},{"location":"kubectl/#install","title":"Install","text":"<p>Follow the install guide from kubernetes.io</p> <p>On windows, you can use choco to install it as well</p> <pre><code>choco install kubernetes-cli\n</code></pre>"},{"location":"kubectl/#configure","title":"Configure","text":"<p>Then configure your configuration by adding extra <code>KUBECONFIG</code> files</p> <p><code>kubectl</code> works with config files. You need to set the <code>KUBECONFIG</code>environment variable to contain all the files you would like to work with.</p> <p></p> <p>Each of the files describes your access to a particular cluster. To get that configuration, ssh into your master node and copy the config from <code>~/.kube/config</code>.</p> <pre><code>scp your-username@192.168.0.200:/home/your-username/.kube/config C:/Users/your-username/.kube/config-esxi-kubernetes\n</code></pre> <p>Make sure that there is no conflicting naming with other config files as each key has to be unique across config files. You could also point the <code>KUBECONFIG</code> to one file and switch contexts using the ENV variable. However when all are loaded, you can switch context with</p> <pre><code>kubectl config get-contexts                          # display list of contexts\nkubectl config current-context                       # display the current-context\nkubectl config use-context my-cluster-name           # set the default context to my-cluster-name\n\n</code></pre> <p>Once you selected the correct config, you should be able to describe the cluster on your local machine</p> <pre><code>kubectl cluster-info\n</code></pre> <p>Ref: connecting to multiple clusters</p>"},{"location":"kubernetes/","title":"Kubernetes","text":""},{"location":"kubernetes/#create-kubernetes-nodes","title":"Create Kubernetes nodes","text":""},{"location":"kubernetes/#prepare-networking","title":"Prepare networking","text":"<p>I added 3 MAC addresses to my router's DHCP settings, so that each machine would get the same IP from the DHCP server</p> <p></p> <p>Next I added 3 new virtual machines, based on the Ubuntu server 18.04 ISO. Each machine was configured the same way, except for the MAC address of course.</p>"},{"location":"kubernetes/#creating-vm-machines","title":"Creating VM machines","text":"<p>Create 3 virtual machine with the following specs:</p> <p></p> <p>Add the Ubuntu ISO to the CD drive, which I downloaded from the site, then uploaded to the datastore.</p> <p>Boot up each machine and just follow the installer of ubuntu server, accepting the defaults. I added the OpenSSH server and imported my github key so I didn't needed to setup ssh access any further.</p> <p> </p> <p>Then SSH into each machine in a separate terminal.</p> <pre><code>ssh 192.168.0.200\nssh 192.168.0.199\nssh 192.168.0.198\n</code></pre> <p>Next verify each machines ip address</p> <pre><code>sudo ip addr\n</code></pre>"},{"location":"kubernetes/#install-software","title":"Install software","text":"<p>Next install docker</p> <pre><code>sudo apt-get update\nsudo apt-get install -y docker.io\n</code></pre> <p>Next installer the kubernetes cli tools</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install -y apt-transport-https curl\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\nsudo cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list\ndeb http://apt.kubernetes.io/ kubernetes-xenial main\nEOF\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\n</code></pre> <p>You need to install these on all the VMs.</p>"},{"location":"kubernetes/#tweak-ubuntu-settings","title":"Tweak Ubuntu Settings","text":"<p>Disabling swap</p> <pre><code>sudo swapoff -a\n</code></pre> <p>Comment out the swap file in <code>/etc/fstab</code></p> <pre><code>sudo nano /etc/fstab\n</code></pre> <p></p> <p>Then delete the swap file</p> <pre><code>sudo rm -f /swap.img\n</code></pre> <pre><code>sudo systemctl enable docker.service\n</code></pre>"},{"location":"kubernetes/#add-dns-entries","title":"Add DNS entries","text":"<pre><code>sudo nano /etc/hosts\n</code></pre> <p>and add the following</p> <pre><code>&lt;vm-ip&gt; &lt;vm-name&gt; &lt;vm-name&gt;.olympus.home\n&lt;vm-ip&gt; rancher rancher.olympus.home\n</code></pre>"},{"location":"kubernetes/#setup-kubernetes","title":"Setup kubernetes","text":""},{"location":"kubernetes/#setup-the-master-node","title":"Setup the master node","text":"<p>On the master node initialize the cluster.</p> <pre><code>sudo kubeadm init\n</code></pre> <p></p> <p>Run the proposed commands on the master</p> <pre><code>mkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p><code>kubeadm</code> does a lot of the heavy lifting required to setup a Kubernetes Cluster like installing a CA, generating certificates, installing and configuring <code>etcd</code>, getting addons like <code>CoreDNS</code>, <code>kube-proxy</code>. But one thing that is does not do is install a networking addon. For Kubernetes to work you need to have a pod network add-on. There are a lot of CNI providers and you can choose any of them. I chose Weave Net as it does not require any additional configuration. You install it on the Master node by running</p> <pre><code>kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\n</code></pre>"},{"location":"kubernetes/#add-the-worker-nodes","title":"Add the worker nodes","text":"<p>Use the following command on each of the worker nodes</p> <pre><code>kubeadm join 192.168.0.200:6443 --token 5auxv4.26************90 --discovery-token-ca-cert-hash sha256:01e5ef2c************************************************6a4ff89564\n</code></pre> <p>If you ever need to reconstruct this command, check out the blog post of Scott Lowe.</p> <p>At this point your Kubernetes Cluster is up and running. You can use the <code>kubectl</code> command from the Master node to query the cluster information.</p> <pre><code>kubectl cluster-info\nkubectl get nodes\n</code></pre> <p></p> <pre><code>kubectl describe nodes\n</code></pre> <p>Once all are in the ready state we have a working Kubernetes cluster with 3 nodes (one master node and 2 worker nodes).</p>"},{"location":"kubernetes/#post-install","title":"Post install","text":""},{"location":"kubernetes/#remote-kubectl","title":"Remote Kubectl","text":"<p>we have <code>kubectl</code> once we ssh into the master node, but we might want to get the same access from an external machine. To do so, we copy the <code>~/.kube/config</code> to our local machine and add it to our local <code>KUBECONFIG</code> env variable.</p> <pre><code>scp your-username@192.168.0.200:/home/your-username/.kube/config C:/Users/your-username/.kube/config-esxi-kubernetes\n</code></pre> <p>More on this in kubectl configuration</p>"},{"location":"kubernetes/#docker-login-for-private-repositories","title":"Docker login for private repositories","text":"<p>To have the cluster be able to pull docker images from a private repository, refer to the documentation.</p> <p>A <code>docker login</code> however will not store the credentials in the <code>config.json</code> as it is insecure, however you can easily create that credential as that's just the base64 encoded string of the username and password for the registry. Note that for github's package registry your password is to be a personal token, not the credentials you use to login.</p> <pre><code>[Convert]::ToBase64String([System.Text.Encoding]::ASCII.GetBytes('$DOCKERHUB_USER:$DOCKERHUB_PASSWORD'))\n</code></pre> <p>or</p> <pre><code>echo -n '$DOCKERHUB_USER:$DOCKERHUB_PASSWD' | base64\n</code></pre> <p>and use the output in <code>config.json</code></p> <pre><code>{\n   \"auths\": {\n        \"https://index.docker.io/v1/\": {\n            \"auth\": \"$BASE64_STRING\"\n        }\n    }\n}\n</code></pre> <p>WARNING Base64 is NOT encryption and should never be considered safe!</p>"},{"location":"kubernetes/#metallb","title":"MetalLb","text":"<p>Since we're runnig on a BareMetal cluster (not on Azure, GCE, AWS, ...), the loadbalancers don't get an IP assigned. Kubernetes doesn't offer that out of the box. A solution for this is MetalLB</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.3/manifests/metallb.yaml\n</code></pre> <p>Then provide a config</p> <pre><code>kubectl apply -f ./src/kubernetes/metallb.yaml\n</code></pre> <p>Now each time a resource of type load balancer is started, MetalLB will assign it an external IP from the pool it was given.</p>"},{"location":"kubernetes/#pull-private-images","title":"Pull private images","text":"<p>Basically follow the kubernetes documentation.</p> <p>Make sure your docker's <code>config.json</code> contains the secret.</p> <pre><code>{\n    \"auths\": {\n        \"https://index.docker.io/v1/\": {\n            \"auth\": \"c3R...zE2\"\n        }\n    }\n}\n</code></pre> <p>NOTE:  In case of github package registry, you use a personal token as the password to give access to your registries.</p> <p>Add a secret with those docker credentials to your cluster and give it a name like <code>regcred</code>.</p> <pre><code>kubectl create secret generic regcred --from-file=.dockerconfigjson=C:/Users/&lt;username&gt;/.docker/config.json --type=kubernetes.io/dockerconfigjson\n</code></pre> <p>Then use that secret by name as part of your deployment</p> <pre><code>spec:\n    containers:\n    image: private-image:1.0\n    imagePullPolicy: IfNotPresent\n    name: private-image\n    imagePullSecrets:\n    - name: regcred\n    restartPolicy: Always\n</code></pre>"},{"location":"kubernetes/#footnotes","title":"Footnotes","text":"<p>Reference</p>"},{"location":"rancher/","title":"Installing rancher","text":""},{"location":"rancher/#install-cert-manager","title":"Install cert-manager","text":"<p>Install the CustomResourceDefinition resources separately</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.10/deploy/manifests/00-crds.yaml\n</code></pre> <p>Create the namespace for cert-manager</p> <pre><code>kubectl create namespace cert-manager\n</code></pre> <p>Label the cert-manager namespace to disable resource validation</p> <pre><code>kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true\n</code></pre> <p>Add the Jetstack Helm repository</p> <pre><code>helm repo add jetstack https://charts.jetstack.io\n</code></pre> <p>Update your local Helm chart repository cache</p> <pre><code>helm repo update\n</code></pre> <p>Install the cert-manager Helm chart</p> <pre><code>helm install --name cert-manager --namespace cert-manager --version v0.10.1 jetstack/cert-manager\n</code></pre> <pre><code>kubectl get pods --namespace cert-manager\n</code></pre>"},{"location":"rancher/#install-rancher","title":"Install rancher","text":""},{"location":"rancher/#add-an-ingress-with-tls","title":"Add an ingress with TLS","text":"<p>Provide an ingress proxy with SSL support. First create the certificate with mkcert (or omgwftssl) for the domain you want. Here we choose <code>rancher.olympus.home</code></p> <p>Do let's set hosts file accordingly on your own machine</p> <ul> <li>Windows:  c:\\windows\\system32\\drivers\\etc\\hosts</li> <li>Mac/Linux: - /etc/hosts</li> </ul> <p>Edit the appropriate file for your system and add an entry</p> <pre><code>&lt;your local ip&gt; rancher.&lt;your domain&gt;\n</code></pre> <p>Mine was</p> <pre><code>192.168.0.200 rancher.olympus.home\n</code></pre> <p>Add the same entries on your VM's in the <code>/etc/hosts</code> file of each VM.</p> <p>Alternatively, if you have a local DNS running, you can add the entry there once (&lt;== prefered).</p> <p>Next generate a certificate with mkcert</p> <pre><code>mkcert rancher.olympus.home localhost 127.0.0.1 ::1\n</code></pre> <p>mkcert will create local CA certificatge with which it'll sign the requested certificate. The cluster will need to know that CA certificate as well to verify its validity.</p> <pre><code>copy %APPDATA%\\mkcert\\rootCA.pem ./cacerts.pem\n</code></pre> <p>Install an ingress proxy controller to expose the future rancher website. The rancher installer will create the ingress pointing to a certificate called <code>cacerts.pem</code>.</p> <pre><code>helm install stable/nginx-ingress --name ingress-nginx --namespace ingress-nginx\n</code></pre> <p>Since we're running on bare-metal, kubernetes has no clue on which infrastructure it is running on. We need to expose the external ip directly</p> <pre><code>kubectl patch svc ingress-nginx-nginx-ingress-controller --namespace=ingress-nginx -p '{\"spec\": {\"type\": \"LoadBalancer\", \"externalIPs\":[\"192.168.0.200\"]}}'\n</code></pre> <p>or get the service from the cluster</p> <pre><code>kubectl get services -o wide ingress-nginx-nginx-ingress-controller --namespace=ingress-nginx -o yaml &gt; ./rancher/ingress-controller.yaml\n</code></pre> <p>Add the external IP manually and apply</p> <pre><code>kubectl apply -f ./rancher/ingress-controller.yaml\n</code></pre>"},{"location":"rancher/#install-rancher_1","title":"Install Rancher","text":"<p>Next we're going to install rancher. We want the alpha version, since we would like to install istio later on.</p> <pre><code>helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\n\nkubectl create namespace cattle-system\n</code></pre> <p>Add the certificates for the installer to find.</p> <pre><code>kubectl -n cattle-system create secret generic tls-ca --from-file=cacerts.pem\n\nkubectl -n cattle-system create secret tls tls-rancher-ingress --key ./rancher.olympus.home+3-key.pem --cert ./rancher.olympus.home+3.pem\n</code></pre> <p>Run the helm chart</p> <pre><code>helm install rancher-latest/rancher --name rancher --namespace cattle-system --set hostname=rancher.olympus.home --set ingress.tls.source=secret --set privateCA=true --version 2.3.2-rc3\n</code></pre> <p>Now open your browser at rancher and create your login.</p> <p></p> <p>Note how the <code>cattle-cluster-agent</code> cannot start properly. That is due to the fact that we use a hostname that is not resolvable from the internet. We didn't use a DNS server to point to this installation.</p> <p>We need to allow the agent to use the node's /etc/hosts file.</p> <p> </p> <p>And save ;)</p>"},{"location":"rancher/#monitoring-istio-install-from-rancher","title":"Monitoring &amp; Istio install from Rancher","text":""},{"location":"rancher/#monitoring-with-prometheus","title":"Monitoring with Prometheus","text":"<p>Adding monitoring through rancher, adds a Prometheus service to scrape the information from the cluster.</p> <p></p> <p>I slimmed down the defaults, since this cluster is not to take the heavy load a cluster usually does. We only use this one for demo purposes. In general go with the defaults for a production environment.</p> <p>With monitoring enabled, we get extra live information.</p> <p></p>"},{"location":"rancher/#istio","title":"Istio","text":"<p>Rancher allows istio to be install automatically given minimum version <code>2.3.0-alpha5</code>. We installed Rancher <code>2.3.2-rc3</code>, so all good there.</p> <p>Next add Istio. For this cluster, divide the default settings roughly by 4. I used the following:</p> <p> </p> <p>Alternatively, you can use <code>./src/rancher/istio-rancher-values.yaml</code> file and use that as an entry for the istio helm deployment under local Project System - Apps - Istio and upgrade it with the new values. These values are taken from the demo values of Istio source code and should suffice for a mini cluster. The minimal values are taking way too much cpu and memory into account and are designed for higher throughput (read production).</p>"}]}